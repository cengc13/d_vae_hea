{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d86922a-d8c2-4374-a0a7-beb7b39fb536",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd ../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d15e281-d7d3-4425-9515-2c23248658b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "from pyro.distributions import Normal\n",
    "from pyro.infer import SVI, Trace_ELBO\n",
    "from torch.optim import Adam\n",
    "from pyro.optim import ReduceLROnPlateau\n",
    "from pyro.contrib.examples.util import print_and_log\n",
    "import pyro.poutine as poutine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9bec5af-1d35-46c5-a2a3-1ca98b9dac2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a760a0bb-df4e-4f3d-9074-865ac79d4cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.custom_mlp import MLP, Exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aff70f5-bd3e-4a0b-951f-77892ea99fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.special import expit\n",
    "import pdb\n",
    "from datetime import datetime\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7db3bf0-4e16-4875-8827-8dac056409a7",
   "metadata": {},
   "source": [
    "**Import composition and engineered data and merge them into a single dataframe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9edb82d7-fb7d-4c39-8984-ad543fd0d9bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_directory = '/work/Roux-EngRes/c.zeng/research/HEA_SVAE/data/'\n",
    "# data_directory = '/data/zulqarnain/hea_single_phase_data/'\n",
    "\n",
    "hea_top30_data = pd.read_csv(data_directory + 'HEA_top30_comps.csv', comment='#')\n",
    "hea_feature_engineered_df = pd.read_csv(data_directory + 'HEA_feature_engineered.csv', comment='#')\n",
    "\n",
    "hea_top30_data = hea_top30_data.drop_duplicates(subset='Alloys', keep='first')\n",
    "hea_feature_engineered_df = hea_feature_engineered_df.drop_duplicates(subset='Alloys', keep='first')\n",
    "\n",
    "merged_df = pd.merge(hea_top30_data, hea_feature_engineered_df.drop(columns='Class'), on='Alloys', how='inner')\n",
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c6ba22-7230-4376-b881-ea7191091951",
   "metadata": {},
   "source": [
    "**Construct Dataset classes to load data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a85df2db-9c19-44df-984a-051687c95360",
   "metadata": {},
   "outputs": [],
   "source": [
    "### this class is for importing composition data, engineered data, and labels ###\n",
    "class HEAFeatureDataset(Dataset): \n",
    "    def __init__(self, pd, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.data = pd\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        labels = np.array(self.data.iloc[idx][\"Class\"], np.float32)\n",
    "        data = np.array(self.data.iloc[idx][\"Fe\":\"Sc\"], np.float32)\n",
    "        data_engineered = np.array(self.data.iloc[idx][\"k\":\"delta_h_mix\"], np.float32)\n",
    "        if self.transform:\n",
    "            data = self.transform(data)\n",
    "\n",
    "        return torch.tensor(data * 100), torch.tensor(data_engineered), torch.tensor(labels).unsqueeze(-1)\n",
    "\n",
    "### this class is for importing just composition data and engineered data, no labels###\n",
    "class HEAFeatureDatasetUnlabelled(Dataset):\n",
    "    def __init__(self, pd, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            pd (DataFrame): pandas dataframe\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.data = pd\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data = np.array(self.data.iloc[idx][\"Fe\":\"Sc\"], np.float32)\n",
    "        data_engineered = np.array(self.data.iloc[idx][\"k\":\"delta_h_mix\"], np.float32)\n",
    "        if self.transform:\n",
    "            data = self.transform(data)\n",
    "\n",
    "        return torch.tensor(data * 100), torch.tensor(data_engineered)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d405e3a3-d57b-4fc0-b3af-d540623c6548",
   "metadata": {},
   "source": [
    "**Define model params**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a53d6482-c2f9-4953-89bd-8d910df8bd73",
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURE_SIZE = 30 # size of composition data\n",
    "BATCH_SIZE = 32 # batch size to use during training\n",
    "DEFAULT_HIDDEN_DIMS = [100,100] \n",
    "DEFAULT_Z_DIM = 2 # size of latent dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e8c2870-9fed-42bf-8a20-52c93f00e3fd",
   "metadata": {},
   "source": [
    "**Split data into train, test and validation sets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "306994e2-09e9-4d3d-b594-eab7285a2fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "labelled_hea, test_hea = train_test_split(merged_df, test_size=0.1, random_state=42) # split into labelled and unlabelled (test) set\n",
    "labelled_hea, unlabelled_hea = train_test_split(labelled_hea, test_size=0.3, random_state=42) # split labelled into labelled train and unlabelled \n",
    "unlabelled_hea, validation_hea = train_test_split(unlabelled_hea, test_size=0.2, random_state=42) # split unlabelled into unlabelled train  \n",
    "                                                                                                    # and unlabelled val\n",
    "## Save data split to pickle files\n",
    "# pickle.dump(labelled_hea, open('./vae_results/labelled_hea.pk', 'wb'))\n",
    "# pickle.dump(unlabelled_hea, open('./vae_results/unlabelled_hea.pk', 'wb'))\n",
    "# pickle.dump(test_hea, open('./vae_results/test_hea.pk', 'wb'))\n",
    "# pickle.dump(validation_hea, open('./vae_results/validation_hea.pk', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae04f66b-14af-4518-bfc6-c9408bbc5c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Reading in dataset...\")\n",
    "\n",
    "hea_feature_dataset = HEAFeatureDataset(pd=labelled_hea)\n",
    "hea_feature_loader = torch.utils.data.DataLoader(hea_feature_dataset,\n",
    "                                                  batch_size=BATCH_SIZE, shuffle=True,)\n",
    "\n",
    "N_samples = len(hea_feature_dataset)\n",
    "print(\"Number of labelled observations:\", N_samples)\n",
    "\n",
    "hea_feature_dataset_ul = HEAFeatureDataset(pd=unlabelled_hea)\n",
    "hea_feature_loader_ul = torch.utils.data.DataLoader(hea_feature_dataset_ul,\n",
    "                                                  batch_size=BATCH_SIZE, shuffle=True,)\n",
    "N_samples = len(hea_feature_dataset_ul)\n",
    "print(\"Number of unlabelled observations:\", N_samples)\n",
    "\n",
    "hea_feature_dataset_val = HEAFeatureDataset(pd=validation_hea)\n",
    "hea_feature_loader_val = torch.utils.data.DataLoader(hea_feature_dataset_val,\n",
    "                                                  batch_size=len(hea_feature_dataset_val), shuffle=True,)\n",
    "N_samples = len(hea_feature_dataset_val)\n",
    "print(\"Number of validation observations:\", N_samples)\n",
    "\n",
    "hea_feature_dataset_test = HEAFeatureDataset(pd=test_hea)\n",
    "hea_feature_loader_test = torch.utils.data.DataLoader(hea_feature_dataset_test,\n",
    "                                                  batch_size=BATCH_SIZE, shuffle=True,)\n",
    "N_samples = len(hea_feature_dataset_test)\n",
    "print(\"Number of test observations:\", N_samples)\n",
    "\n",
    "data_loaders = {\"sup\": hea_feature_loader , \"unsup\": hea_feature_loader_ul, \"val\": hea_feature_loader_val, \"test\": hea_feature_loader_test}\n",
    "### \"sup\" is for supervised training data, \"unsup\" is for unsupervised training data, \"val\" is for validation, \"test\" is for test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02917e01-a821-4916-b226-29f0edf6d123",
   "metadata": {},
   "source": [
    "**Define Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe9bcc4-4c56-455e-ab80-36a634f87f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SSVAE(nn.Module):\n",
    "    \"\"\"\n",
    "    The semi-supervised VAE\n",
    "\n",
    "    :param output_size: size of the tensor representing the class label (1 for our data, as it's binary)\n",
    "    :param input_size: size of the tensor representing the input (30 for our data)\n",
    "    :param z_dim: size of the tensor representing the latent random variable z\n",
    "    :param hidden_layers: a tuple (or list) of MLP layers to be used in the neural networks\n",
    "                          representing the parameters of the distributions in our model\n",
    "    :param use_cuda: use GPUs for faster training\n",
    "    :param aux_loss_multiplier: the multiplier to use with the auxiliary loss\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        output_size=1,\n",
    "        input_size=30,\n",
    "        z_dim=2,\n",
    "        hidden_layers=(500,),\n",
    "        config_enum=None,\n",
    "        use_cuda=False,\n",
    "        aux_loss_multiplier=None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # initialize the class with all arguments provided to the constructor\n",
    "        self.output_size = output_size\n",
    "        self.input_size = input_size\n",
    "        self.z_dim = z_dim\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.allow_broadcast = config_enum == \"parallel\"\n",
    "        self.use_cuda = use_cuda\n",
    "        self.aux_loss_multiplier = aux_loss_multiplier\n",
    "\n",
    "        # define and instantiate the neural networks representing\n",
    "        # the parameters of various distributions in the model\n",
    "        self.setup_networks()\n",
    "\n",
    "    def setup_networks(self):\n",
    "        z_dim = self.z_dim\n",
    "        hidden_sizes = self.hidden_layers\n",
    "\n",
    "        # Encoder for y outputs a single probability per instance (Bernoulli parameter)\n",
    "        self.encoder_y = MLP(\n",
    "            [8] + hidden_sizes + [1],  # Output size is 1 for Bernoulli probability\n",
    "            activation=nn.Softplus,\n",
    "            output_activation=nn.Sigmoid,  # Sigmoid activation for a valid probability\n",
    "            allow_broadcast=self.allow_broadcast,\n",
    "            use_cuda=self.use_cuda,\n",
    "        )\n",
    "\n",
    "        # Encoder for z outputs parameters for a normal distribution\n",
    "        # Final output size is [z_dim, z_dim] for mean and std dev, activations are None and Exp (for std)\n",
    "        self.encoder_z = MLP(\n",
    "            [self.input_size + 1] + hidden_sizes + [[z_dim, z_dim]],  # Added +1 for binary y\n",
    "            activation=nn.Softplus,\n",
    "            output_activation=[None, Exp],  # Exp to ensure positive standard deviation\n",
    "            allow_broadcast=self.allow_broadcast,\n",
    "            use_cuda=self.use_cuda,\n",
    "        )\n",
    "\n",
    "        # Decoder outputs probabilities for a multinomial distribution\n",
    "        # Assuming the total number of categories in x is defined by input_size\n",
    "        self.decoder = MLP(\n",
    "            [z_dim + 1] + hidden_sizes + [self.input_size],  # Adjust input for binary y\n",
    "            activation=nn.Softplus,\n",
    "            output_activation=nn.Softmax(dim=-1),  # Softmax to create a probability distribution\n",
    "            allow_broadcast=self.allow_broadcast,\n",
    "            use_cuda=self.use_cuda,\n",
    "        )\n",
    "\n",
    "        if self.use_cuda:\n",
    "            self.cuda()\n",
    "\n",
    "    def model(self, xs, es=None, ys=None):\n",
    "        \"\"\"\n",
    "        The model corresponds to the following generative process:\n",
    "        p(z) = normal(0,I)              # prior on latents\n",
    "        p(y|x) = Bernoulli(1/2.)     # which phase (semi-supervised)\n",
    "        p(x|y,z) = Multinomial(loc(y,z))   # output composition\n",
    "        loc is given by a neural network  `decoder`\n",
    "\n",
    "        :param xs: a batch of composition data\n",
    "        :param ys: (optional) a batch of the class labels i.e.\n",
    "                   phase corresponding to a given composition\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        # register this pytorch module and all of its sub-modules with pyro\n",
    "        pyro.module(\"ss_vae\", self)\n",
    "\n",
    "        batch_size = xs.size(0)\n",
    "        options = dict(dtype=xs.dtype, device=xs.device)\n",
    "        with pyro.plate(\"data\"):\n",
    "            # sample the latents from the constant prior distribution\n",
    "            # prior_loc = torch.zeros(batch_size, self.z_dim, **options)\n",
    "            prior_loc = torch.zeros(batch_size, self.z_dim, device=device)\n",
    "            # prior_scale = torch.ones(batch_size, self.z_dim, **options)\n",
    "            prior_scale = torch.ones(batch_size, self.z_dim, device=device)\n",
    "            zs = pyro.sample(\"z\", dist.Normal(prior_loc, prior_scale).to_event(1))\n",
    "\n",
    "            # if the label y (which phase) is unsupervised, sample from the\n",
    "            # constant prior, otherwise, observe the value (i.e. score it against the constant prior)\n",
    "            ys_prior_mean  = torch.ones(size=[batch_size, self.output_size], device=device) *0.5\n",
    "            if ys is None:\n",
    "                ys = pyro.sample(\"y\", dist.Bernoulli(probs=ys_prior_mean).to_event(1))\n",
    "            else:\n",
    "                ys = pyro.sample(\"y\", dist.Bernoulli(probs=ys_prior_mean).to_event(1), obs=ys)\n",
    "\n",
    "            # Finally, score the composition data (x) using the latent (z) and\n",
    "            # the class label y (which phase) against the\n",
    "            # parametrized distribution p(x|y,z) = Multinomial(decoder(y,z))\n",
    "            # where `decoder` is a neural network.\n",
    "            loc = self.decoder([zs, ys])\n",
    "            pyro.sample(\"x\", dist.Multinomial(total_count=101, probs=loc), obs=xs)\n",
    "            # return the loc so we can visualize it later\n",
    "            return loc\n",
    "\n",
    "    def guide(self, xs, es=None, ys=None):\n",
    "        \"\"\"\n",
    "        The guide corresponds to the following:\n",
    "        q(y|x) = Bernoulli(probs(f(x)))              # infer phase from engineered features for composition x\n",
    "        q(z|x,y) = normal(loc(x,y),scale(x,y))       # infer latents from composition and the phase\n",
    "        loc, scale are given by a neural network `encoder_z`\n",
    "        probs is given by a neural network `encoder_y`\n",
    "\n",
    "        :param xs: a batch of composition data\n",
    "        :param es: a batch of engineered features for xs i.e. f(x) \n",
    "        :param ys: (optional) a batch of the class labels i.e.\n",
    "                   the phase corresponding to the composition(s)\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        # inform Pyro that the variables in the batch of xs, ys are conditionally independent\n",
    "        with pyro.plate(\"data\"):\n",
    "            # if the class label (the phase) is not supervised, sample\n",
    "            # (and score) the phase with the variational distribution\n",
    "            # q(y|x) =  Bernoulli(probs(f(x)))\n",
    "            if ys is None:\n",
    "                probs = self.encoder_y(es)\n",
    "                ys = pyro.sample(\"y\", dist.Bernoulli(probs=probs).to_event(1))\n",
    "\n",
    "            # sample (and score) the latent  with the variational\n",
    "            # distribution q(z|x,y) = normal(loc(x,y),scale(x,y))\n",
    "            loc, scale = self.encoder_z([xs, ys])\n",
    "            pyro.sample(\"z\", dist.Normal(loc, scale).to_event(1))\n",
    "\n",
    "    def classifier(self, es):\n",
    "        \"\"\"\n",
    "        classify engineered features of a composition (or a batch)\n",
    "\n",
    "        :param es: a batch of engineered features for a composition\n",
    "        :return: a batch of the corresponding class labels\n",
    "        \"\"\"\n",
    "        # use the trained model q(y|x) = Bernoulli(probs(f(x)))\n",
    "        \n",
    "        alpha = self.encoder_y(es)\n",
    "        ys = (alpha > 0.5).float()\n",
    "        \n",
    "        return ys\n",
    "\n",
    "    def model_classify(self, xs, es, ys=None):\n",
    "        \"\"\"\n",
    "        this model is used to add an auxiliary (supervised) loss as described in the\n",
    "        Kingma et al., \"Semi-Supervised Learning with Deep Generative Models\".\n",
    "        \"\"\"\n",
    "        # register all pytorch (sub)modules with pyro\n",
    "        pyro.module(\"ss_vae\", self)\n",
    "\n",
    "        # inform Pyro that the variables in the batch of xs, ys are conditionally independent\n",
    "        with pyro.plate(\"data\"):\n",
    "            # this here is the extra term to yield an auxiliary loss that we do gradient descent on\n",
    "            if ys is not None:\n",
    "                probs = self.encoder_y(es)\n",
    "                with pyro.poutine.scale(scale=self.aux_loss_multiplier):\n",
    "                    pyro.sample(\"y_aux\", dist.Bernoulli(probs=probs).to_event(1), obs=ys)\n",
    "\n",
    "    def guide_classify(self, xs, es, ys=None):\n",
    "        \"\"\"\n",
    "        dummy guide function to accompany model_classify in inference\n",
    "        \"\"\"\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "163d8282-2a9e-442d-bf53-8557f06b48fa",
   "metadata": {},
   "source": [
    "**Model Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85785f91-1a83-43f2-801d-913e77bc082c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference_for_epoch(data_loaders, losses, epoch, gamma=1e-2, c=800, cuda=False):\n",
    "    \"\"\"\n",
    "    runs the inference algorithm for an epoch\n",
    "    returns the values of all losses separately on supervised and unsupervised parts\n",
    "    \"\"\"\n",
    "    num_losses = len(losses)\n",
    "\n",
    "    # compute number of batches for an epoch\n",
    "    # don't use all the sup_batches\n",
    "    sup_batches = len(data_loaders[\"sup\"])\n",
    "    #sup_batches = len(data_loaders[\"supervised\"])\n",
    "    unsup_batches = len(data_loaders[\"unsup\"])\n",
    "    batches_per_epoch = sup_batches + unsup_batches\n",
    "\n",
    "    # initialize variables to store loss values\n",
    "    epoch_losses_sup = [0.] * num_losses\n",
    "    epoch_losses_unsup = [0.] * num_losses\n",
    "\n",
    "    # setup the iterators for training data loaders\n",
    "    sup_iter = iter(data_loaders[\"sup\"])\n",
    "    unsup_iter = iter(data_loaders[\"unsup\"])\n",
    "\n",
    "    # random order\n",
    "    is_sups = [1]*sup_batches + [0]*unsup_batches\n",
    "    is_sups = np.random.permutation(is_sups)\n",
    "\n",
    "    beta = 1\n",
    "    for i in range(batches_per_epoch):\n",
    "        is_supervised = is_sups[i]\n",
    "\n",
    "        # extract the corresponding batch\n",
    "        if is_supervised:\n",
    "            xs, es, ys = next(sup_iter)\n",
    "        else:\n",
    "            xs, es, ys = next(unsup_iter)\n",
    "        if cuda:\n",
    "            ys = ys.cuda()\n",
    "            xs = xs.cuda()\n",
    "            es = es.cuda()\n",
    "\n",
    "        batchsize = xs.size(0)\n",
    "        xs = xs.view(batchsize, -1)\n",
    "        es = es.view(batchsize, -1)\n",
    "        # run the inference for each loss with supervised or un-supervised\n",
    "        # data as arguments\n",
    "        for loss_id in range(num_losses):\n",
    "            if is_supervised:\n",
    "                new_loss = losses[loss_id].step(xs, es, ys) #, beta=beta)\n",
    "                epoch_losses_sup[loss_id] += new_loss\n",
    "            else:\n",
    "                new_loss = losses[loss_id].step(xs, es) #, beta=beta)\n",
    "                epoch_losses_unsup[loss_id] += new_loss\n",
    "\n",
    "    # return the values of all losses\n",
    "    return epoch_losses_sup, epoch_losses_unsup\n",
    "\n",
    "def evaluate_model(data_loader, model, losses, cuda=False):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    num_losses = len(losses)\n",
    "\n",
    "    # compute number of batches for an epoch\n",
    "    batches_per_epoch = len(data_loader)\n",
    "\n",
    "    # initialize variables to store loss values\n",
    "    epoch_losses_sup = [0.] * num_losses\n",
    "    # setup the iterators for training data loaders\n",
    "    sup_iter = iter(data_loader)\n",
    "\n",
    "    for i in range(batches_per_epoch):\n",
    "        xs, es, ys = next(sup_iter)\n",
    "        if cuda:\n",
    "            ys = ys.cuda()\n",
    "            xs = xs.cuda()\n",
    "            es = es.cuda()\n",
    "\n",
    "        batchsize = xs.size(0)\n",
    "        xs = xs.view(batchsize, -1)\n",
    "        es = es.view(batchsize, -1)\n",
    "        # run the inference for each loss with supervised or un-supervised\n",
    "        # data as arguments\n",
    "        for loss_id in range(num_losses):\n",
    "            new_loss = losses[loss_id].step(xs, es, ys) #, beta=beta)\n",
    "            epoch_losses_sup[loss_id] += new_loss\n",
    "\n",
    "    # return the values of all losses\n",
    "    return epoch_losses_sup\n",
    "\n",
    "def get_accuracy(data_loader, classifier_fn, batch_size, cuda=False):\n",
    "    \"\"\"\n",
    "    compute the accuracy over the supervised training set or the testing set\n",
    "    \"\"\"\n",
    "    predictions, actuals = [], []\n",
    "\n",
    "    # use the appropriate data loader\n",
    "    for xs, es, ys in data_loader:\n",
    "        if cuda:\n",
    "            ys = ys.cuda()\n",
    "            xs = xs.cuda()\n",
    "            es = es.cuda()\n",
    "        # use classification function to compute all predictions for each batch\n",
    "        predictions.append(classifier_fn(es))\n",
    "        actuals.append(ys)\n",
    "    actuals = torch.cat(actuals, dim=0).squeeze()\n",
    "    predictions = torch.cat(predictions, dim=0).squeeze()\n",
    "    \n",
    "    accuracy = (actuals == predictions).float().mean()\n",
    "    \n",
    "    return accuracy.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3237cd43-46ce-4bb4-8db0-de5d06d36b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    learning_rate = 1e-4\n",
    "    num_epochs = 20000 #1000\n",
    "    hidden_layers = DEFAULT_HIDDEN_DIMS\n",
    "    z_dim = DEFAULT_Z_DIM\n",
    "    beta_1 = 0.900\n",
    "    aux_loss = True\n",
    "    aux_loss_multiplier = 10 #50.0\n",
    "    # cuda = True\n",
    "    cuda = False # turn on cuda if GPU is available\n",
    "\n",
    "args = Args()\n",
    "\n",
    "\n",
    "pyro.clear_param_store()\n",
    "# if __name__ == '__main__':\n",
    "unsup_num = len(hea_feature_dataset_ul)\n",
    "sup_num = len(hea_feature_dataset)\n",
    "val_num = len(hea_feature_dataset_val)\n",
    "\n",
    "ssvae = SSVAE(output_size=1, input_size=FEATURE_SIZE,\n",
    "              z_dim=args.z_dim,\n",
    "              hidden_layers=args.hidden_layers,\n",
    "              use_cuda=args.cuda,\n",
    "              #config_enum=args.enum_discrete,\n",
    "              aux_loss_multiplier=args.aux_loss_multiplier)\n",
    "\n",
    "# setup the optimizer\n",
    "adam_params = {\"lr\": args.learning_rate, \"betas\": (args.beta_1, 0.999)}\n",
    "optimizer = Adam #(adam_params)\n",
    "\n",
    "scheduler = ReduceLROnPlateau({'optimizer': optimizer, 'optim_args': adam_params, \n",
    "                               'mode': 'min', 'factor': 0.5, 'patience': 200, 'verbose': True})\n",
    "loss_basic = SVI(ssvae.model, ssvae.guide, scheduler, loss=Trace_ELBO())\n",
    "# build a list of all losses considered\n",
    "losses = [loss_basic]\n",
    "\n",
    "# aux_loss: whether to use the auxiliary loss from (Kingma et al, 2014 NIPS paper)\n",
    "if args.aux_loss:\n",
    "    loss_aux = SVI(ssvae.model_classify, ssvae.guide_classify, scheduler, loss=Trace_ELBO())\n",
    "    losses.append(loss_aux)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c8c24cc-c983-4952-84d5-c09e0ac07459",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_val_accuracy = 0\n",
    "best_train_accuracy = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7be68b5-9f24-4d0c-b857-fddbe0ebe409",
   "metadata": {},
   "source": [
    "**1-3-2** in file suffix refers to how the data was split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab640cd-e25c-47a1-8c68-2aaedd58ca90",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "try:\n",
    "    # run inference for a certain number of epochs\n",
    "    for i in range(0, args.num_epochs):\n",
    "\n",
    "        # get the losses for an epoch\n",
    "        epoch_losses_sup, epoch_losses_unsup = \\\n",
    "            run_inference_for_epoch(data_loaders, losses, epoch=i, cuda=args.cuda)\n",
    "\n",
    "        epoch_losses_val = evaluate_model(data_loaders[\"val\"], ssvae, losses, cuda=args.cuda)\n",
    "        avg_epoch_losses_val = map(lambda v: v / val_num, epoch_losses_val)\n",
    "\n",
    "        str_loss_val = \" \".join(map(str, avg_epoch_losses_val))\n",
    "        ssvae.train()\n",
    "        # compute average epoch losses i.e. losses per example\n",
    "        avg_epoch_losses_sup = map(lambda v: v / sup_num, epoch_losses_sup)\n",
    "        avg_epoch_losses_unsup = map(lambda v: v / unsup_num, epoch_losses_unsup)\n",
    "\n",
    "        # store the loss and validation/testing accuracies in the logfile\n",
    "        str_loss_sup = \" \".join(map(str, avg_epoch_losses_sup))\n",
    "        str_loss_unsup = \" \".join(map(str, avg_epoch_losses_unsup))\n",
    "\n",
    "        str_print = \"Epoch {} : Avg {}\".format(i, \"{} {}\".format(str_loss_sup, str_loss_unsup))\n",
    "        torch.nn.utils.clip_grad_norm_(ssvae.encoder_z.parameters(), max_norm=1.0)\n",
    "        training_accuracy = get_accuracy(\n",
    "            data_loaders[\"sup\"], ssvae.classifier, BATCH_SIZE, cuda=args.cuda\n",
    "        )\n",
    "        if training_accuracy > best_train_accuracy:\n",
    "            best_train_accuracy = training_accuracy\n",
    "            torch.save(ssvae.state_dict(), \"./vae_results/ssvae_best_train_1-3-2.model\")\n",
    "        str_print += \" training accuracy {}\".format(training_accuracy)\n",
    "        print(str_print)\n",
    "        # this test accuracy is only for logging, this is not used\n",
    "        # to make any decisions during training\n",
    "        validation_accuracy = get_accuracy(\n",
    "            data_loaders[\"val\"], ssvae.classifier, BATCH_SIZE, cuda=args.cuda\n",
    "        )\n",
    "        if validation_accuracy > best_val_accuracy:\n",
    "            best_val_accuracy = validation_accuracy\n",
    "            torch.save(ssvae.state_dict(), \"./vae_results/ssvae_best_val_1-3-2.model\")    \n",
    "        str_print = \"Epoch {} : Avg {}\".format(i, \"{}\".format(str_loss_val))\n",
    "        str_print += \" validation accuracy {}\".format(validation_accuracy)\n",
    "\n",
    "        # predErr, _, _ = get_prediction_error(data_loaders[\"supervised\"], ssvae.rate)\n",
    "        # str_print += \"\\n     Train set prediction error: {}\".format(predErr)\n",
    "        print(str_print)\n",
    "        scheduler.step(epoch_losses_sup[0] / sup_num)\n",
    "        \n",
    "        if i % 1000 == 0:\n",
    "            torch.save(ssvae.state_dict(), f\"./vae_results/ssvae_1-3-2_{i}.model\")    \n",
    "            \n",
    "finally:\n",
    "    print(\"Done\")\n",
    "    torch.save(ssvae.state_dict(), \"./vae_results/ssvae_1-3-2\" + str(datetime.now().strftime('%m%d%Y_%H%M%S')) + \".model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39271a62-4784-49e3-a0bc-5c3132c082ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_loss = evaluate_model(data_loaders[\"val\"], ssvae, losses, cuda=args.cuda)\n",
    "print (validation_loss)\n",
    "np.array(validation_loss)/ len(hea_feature_dataset_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c414b981-bb34-4e4f-a4ed-18f0c0e88f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "ssvae.eval() \n",
    "test_accuracy = get_accuracy(\n",
    "    data_loaders[\"test\"], ssvae.classifier, len(hea_feature_dataset_test), cuda=args.cuda\n",
    ")\n",
    "# str_print = \"Epoch {} : Avg {}\".format(i, \"{}\".format(str_loss_val))\n",
    "print (\" test accuracy {}\".format(test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea06960-60e7-4ffa-a855-5ed1f2d54843",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a39f8c50-5f47-423b-87ae-cbce1ac89c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_latent(z_loc, classes, name, tsne=False):\n",
    "    import matplotlib\n",
    "\n",
    "    # matplotlib.use(\"Agg\")\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    from sklearn.manifold import TSNE\n",
    "    \n",
    "    if tsne:\n",
    "        model_tsne = TSNE(n_components=2, random_state=0)\n",
    "        z_states = z_loc.detach().cpu().numpy()\n",
    "        z_embed = model_tsne.fit_transform(z_states)\n",
    "        classes = classes.detach().cpu().numpy()\n",
    "    else:\n",
    "        z_embed = z_loc.detach().cpu().numpy()\n",
    "        classes = classes.detach().cpu().numpy()\n",
    "    fig = plt.figure()\n",
    "    for (z, l) in zip(z_embed, classes):\n",
    "        color = plt.cm.Set1(l)\n",
    "        plt.scatter(z[0], z[1], s=10, color=color)\n",
    "        plt.title(\"Latent Variable T-SNE per Class\")\n",
    "#         fig.savefig(\"./vae_results/\" + str(name) + \"_embedding_\" + str(ic) + \".png\")\n",
    "    fig.savefig(\"./vae_results/\" + str(name) + \"_embedding.png\")\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d05e3f-1df0-4035-9cf0-53d592787cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"SSVAE30_1-3-2_test\" + str(datetime.now().strftime('%m%d%Y_%H%M%S'))\n",
    "for (data, _, labels) in torch.utils.data.DataLoader(hea_feature_dataset_test,\n",
    "                                                  batch_size=len(hea_feature_dataset_test), shuffle=True,\n",
    "                                                  ):\n",
    "#     print (data)\n",
    "    z_loc, z_scale = ssvae.encoder_z([data, labels])\n",
    "    plot_latent(z_loc, labels, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d3767e-8a0c-41e5-98df-ec990bfb7ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "loc = ssvae.decoder([z_loc, labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d57663-d41d-4aea-bcdf-a96a79d1639d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data # test composition data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced7eb75-8725-4b58-bf1d-80dd30daa160",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.round(loc, decimals=2)*100 #reconstructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19bc748e-b369-4f50-bf55-d899f0d06da9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
