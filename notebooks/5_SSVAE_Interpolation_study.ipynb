{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "996bff3e-5a12-452d-81e7-d5bfdfe567ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/hughes/usr/d_vae_hea\n"
     ]
    }
   ],
   "source": [
    "%cd ../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ee50200-6db0-4140-b76e-5d4c7c5fddd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "from pyro.distributions import Normal\n",
    "from pyro.infer import SVI, Trace_ELBO\n",
    "from torch.optim import Adam\n",
    "from pyro.optim import ReduceLROnPlateau\n",
    "from pyro.contrib.examples.util import print_and_log\n",
    "import pyro.poutine as poutine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9fc33749-92ec-4e15-b7a0-4f29e3091df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.custom_mlp import MLP, Exp\n",
    "from utils.featurization import top30, calculate_compositions, calculate_engineered_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "comprehensive-shade",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scikitplot as skplt\n",
    "import matplotlib\n",
    "matplotlib.rc('font', **{'size': 18})\n",
    "from matplotlib.patches import Patch\n",
    "from matplotlib.lines import Line2D\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2aff70f5-bd3e-4a0b-951f-77892ea99fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.special import expit\n",
    "import pdb\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "import pickle\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f5ae50d-952d-45c2-a209-73915aac63dd",
   "metadata": {},
   "source": [
    "**Import composition and engineered data and merge them into a single dataframe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f19748ae-19c5-4777-bc7c-e09622bc6e8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Alloys</th>\n",
       "      <th>Class</th>\n",
       "      <th>Fe</th>\n",
       "      <th>Ni</th>\n",
       "      <th>Cr</th>\n",
       "      <th>Co</th>\n",
       "      <th>Al</th>\n",
       "      <th>Ti</th>\n",
       "      <th>Cu</th>\n",
       "      <th>Mo</th>\n",
       "      <th>...</th>\n",
       "      <th>Li</th>\n",
       "      <th>Sc</th>\n",
       "      <th>k</th>\n",
       "      <th>vm</th>\n",
       "      <th>tm</th>\n",
       "      <th>vac</th>\n",
       "      <th>delta</th>\n",
       "      <th>delta_chi</th>\n",
       "      <th>delta_s_mix</th>\n",
       "      <th>delta_h_mix</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mg1Ni1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>112.0</td>\n",
       "      <td>10.300</td>\n",
       "      <td>1325.65</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.263158</td>\n",
       "      <td>0.300</td>\n",
       "      <td>5.762824</td>\n",
       "      <td>-8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Mg1Cu1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>92.5</td>\n",
       "      <td>10.550</td>\n",
       "      <td>1140.46</td>\n",
       "      <td>6.5</td>\n",
       "      <td>5.263158</td>\n",
       "      <td>0.295</td>\n",
       "      <td>5.762824</td>\n",
       "      <td>-6.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Fe1Mg1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>107.0</td>\n",
       "      <td>10.550</td>\n",
       "      <td>1367.15</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.448276</td>\n",
       "      <td>0.260</td>\n",
       "      <td>5.762824</td>\n",
       "      <td>23.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Cr1Mg1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>102.5</td>\n",
       "      <td>10.615</td>\n",
       "      <td>1551.65</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.448276</td>\n",
       "      <td>0.175</td>\n",
       "      <td>5.762824</td>\n",
       "      <td>31.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Mg1V1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>11.175</td>\n",
       "      <td>1553.15</td>\n",
       "      <td>3.5</td>\n",
       "      <td>5.263158</td>\n",
       "      <td>0.160</td>\n",
       "      <td>5.762824</td>\n",
       "      <td>30.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Alloys  Class   Fe   Ni   Cr   Co   Al   Ti   Cu   Mo  ...   Li   Sc  \\\n",
       "0  Mg1Ni1      0  0.0  0.5  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0   \n",
       "1  Mg1Cu1      0  0.0  0.0  0.0  0.0  0.0  0.0  0.5  0.0  ...  0.0  0.0   \n",
       "2  Fe1Mg1      0  0.5  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0   \n",
       "3  Cr1Mg1      0  0.0  0.0  0.5  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0   \n",
       "4   Mg1V1      0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0   \n",
       "\n",
       "       k      vm       tm  vac     delta  delta_chi  delta_s_mix  delta_h_mix  \n",
       "0  112.0  10.300  1325.65  6.0  5.263158      0.300     5.762824         -8.0  \n",
       "1   92.5  10.550  1140.46  6.5  5.263158      0.295     5.762824         -6.7  \n",
       "2  107.0  10.550  1367.15  5.0  3.448276      0.260     5.762824         23.2  \n",
       "3  102.5  10.615  1551.65  4.0  3.448276      0.175     5.762824         31.7  \n",
       "4  102.0  11.175  1553.15  3.5  5.263158      0.160     5.762824         30.2  \n",
       "\n",
       "[5 rows x 40 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_directory = './data/'\n",
    "\n",
    "hea_top30_data = pd.read_csv(data_directory + 'HEA_top30_comps.csv', comment='#')\n",
    "hea_feature_engineered_df = pd.read_csv(data_directory + 'HEA_feature_engineered.csv')\n",
    "\n",
    "hea_top30_data = hea_top30_data.drop_duplicates(subset='Alloys', keep='first')\n",
    "hea_feature_engineered_df = hea_feature_engineered_df.drop_duplicates(subset='Alloys', keep='first')\n",
    "\n",
    "merged_df = pd.merge(hea_top30_data, hea_feature_engineered_df.drop(columns='Class'), on='Alloys', how='inner')\n",
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e0cbc2-b39d-4253-89e9-03b820bb3a12",
   "metadata": {},
   "source": [
    "**Construct Dataset classes to load data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a85df2db-9c19-44df-984a-051687c95360",
   "metadata": {},
   "outputs": [],
   "source": [
    "### this class is for importing composition data, engineered data, and labels ###\n",
    "class HEAFeatureDataset(Dataset): \n",
    "    def __init__(self, pd, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.data = pd\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        labels = np.array(self.data.iloc[idx][\"Class\"], np.float32)\n",
    "        data = np.array(self.data.iloc[idx][\"Fe\":\"Sc\"], np.float32)\n",
    "        data_engineered = np.array(self.data.iloc[idx][\"k\":\"delta_h_mix\"], np.float32)\n",
    "        if self.transform:\n",
    "            data = self.transform(data)\n",
    "\n",
    "        return torch.tensor(data * 100), torch.tensor(data_engineered), torch.tensor(labels).unsqueeze(-1)\n",
    "\n",
    "### this class is for importing just composition data and engineered data, no labels###\n",
    "class HEAFeatureDatasetUnlabelled(Dataset):\n",
    "    def __init__(self, pd, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            pd (DataFrame): pandas dataframe\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.data = pd\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data = np.array(self.data.iloc[idx][\"Fe\":\"Sc\"], np.float32)\n",
    "        data_engineered = np.array(self.data.iloc[idx][\"k\":\"delta_h_mix\"], np.float32)\n",
    "        if self.transform:\n",
    "            data = self.transform(data)\n",
    "\n",
    "        return torch.tensor(data * 100), torch.tensor(data_engineered)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "648df80f-074c-44db-972c-60f43bd7436e",
   "metadata": {},
   "source": [
    "**load train, test, val dataframes as used during training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c20856a4-373a-429f-a6fc-d4b77d41f115",
   "metadata": {},
   "outputs": [],
   "source": [
    "labelled_hea = pickle.load(open('./data/labelled_hea.pk', 'rb'))\n",
    "unlabelled_hea = pickle.load(open('./data/unlabelled_hea.pk', 'rb'))\n",
    "test_hea = pickle.load(open('./data/test_hea.pk', 'rb'))\n",
    "validation_hea = pickle.load(open('./data/validation_hea.pk', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "229c991b-4790-45b2-a36f-0098f971574d",
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURE_SIZE = 30\n",
    "BATCH_SIZE = 32\n",
    "DEFAULT_HIDDEN_DIMS = [100,100]\n",
    "DEFAULT_Z_DIM = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f01419de-10e9-41a7-87d0-48e686c72336",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading in dataset...\n",
      "Number of labelled observations: 864\n",
      "Number of unlabelled observations: 296\n",
      "Number of validation observations: 75\n",
      "Number of test observations: 138\n"
     ]
    }
   ],
   "source": [
    "print(\"Reading in dataset...\")\n",
    "\n",
    "hea_feature_dataset = HEAFeatureDataset(pd=labelled_hea)\n",
    "hea_feature_loader = torch.utils.data.DataLoader(hea_feature_dataset,\n",
    "                                                  batch_size=BATCH_SIZE, shuffle=True,)\n",
    "\n",
    "N_samples = len(hea_feature_dataset)\n",
    "print(\"Number of labelled observations:\", N_samples)\n",
    "\n",
    "hea_feature_dataset_ul = HEAFeatureDataset(pd=unlabelled_hea)\n",
    "hea_feature_loader_ul = torch.utils.data.DataLoader(hea_feature_dataset_ul,\n",
    "                                                  batch_size=BATCH_SIZE, shuffle=True,)\n",
    "N_samples = len(hea_feature_dataset_ul)\n",
    "print(\"Number of unlabelled observations:\", N_samples)\n",
    "\n",
    "hea_feature_dataset_val = HEAFeatureDataset(pd=validation_hea)\n",
    "hea_feature_loader_val = torch.utils.data.DataLoader(hea_feature_dataset_val,\n",
    "                                                  batch_size=len(hea_feature_dataset_val), shuffle=True,)\n",
    "N_samples = len(hea_feature_dataset_val)\n",
    "print(\"Number of validation observations:\", N_samples)\n",
    "\n",
    "hea_feature_dataset_test = HEAFeatureDataset(pd=test_hea)\n",
    "hea_feature_loader_test = torch.utils.data.DataLoader(hea_feature_dataset_test,\n",
    "                                                  batch_size=BATCH_SIZE, shuffle=True,)\n",
    "N_samples = len(hea_feature_dataset_test)\n",
    "print(\"Number of test observations:\", N_samples)\n",
    "\n",
    "data_loaders = {\"sup\": hea_feature_loader , \"unsup\": hea_feature_loader_ul, \"val\": hea_feature_loader_val, \"test\": hea_feature_loader_test}\n",
    "### \"sup\" is for supervised training data, \"unsup\" is for unsupervised training data, \"val\" is for validation, \"test\" is for test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a37e2f-3aa2-47c1-a51f-b4bb17abc154",
   "metadata": {},
   "source": [
    "**Define Model, this is the same as training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bbe9bcc4-4c56-455e-ab80-36a634f87f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SSVAE(nn.Module):\n",
    "    \"\"\"\n",
    "    The semi-supervised VAE\n",
    "\n",
    "    :param output_size: size of the tensor representing the class label (1 for our data, as it's binary)\n",
    "    :param input_size: size of the tensor representing the input (30 for our data)\n",
    "    :param z_dim: size of the tensor representing the latent random variable z\n",
    "    :param hidden_layers: a tuple (or list) of MLP layers to be used in the neural networks\n",
    "                          representing the parameters of the distributions in our model\n",
    "    :param use_cuda: use GPUs for faster training\n",
    "    :param aux_loss_multiplier: the multiplier to use with the auxiliary loss\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        output_size=1,\n",
    "        input_size=30,\n",
    "        z_dim=2,\n",
    "        hidden_layers=(500,),\n",
    "        config_enum=None,\n",
    "        use_cuda=False,\n",
    "        aux_loss_multiplier=None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # initialize the class with all arguments provided to the constructor\n",
    "        self.output_size = output_size\n",
    "        self.input_size = input_size\n",
    "        self.z_dim = z_dim\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.allow_broadcast = config_enum == \"parallel\"\n",
    "        self.use_cuda = use_cuda\n",
    "        self.aux_loss_multiplier = aux_loss_multiplier\n",
    "\n",
    "        # define and instantiate the neural networks representing\n",
    "        # the parameters of various distributions in the model\n",
    "        self.setup_networks()\n",
    "\n",
    "    def setup_networks(self):\n",
    "        z_dim = self.z_dim\n",
    "        hidden_sizes = self.hidden_layers\n",
    "\n",
    "        # Encoder for y outputs a single probability per instance (Bernoulli parameter)\n",
    "        self.encoder_y = MLP(\n",
    "            [8] + hidden_sizes + [1],  # Output size is 1 for Bernoulli probability\n",
    "            activation=nn.Softplus,\n",
    "            output_activation=nn.Sigmoid,  # Sigmoid activation for a valid probability\n",
    "            allow_broadcast=self.allow_broadcast,\n",
    "            use_cuda=self.use_cuda,\n",
    "        )\n",
    "\n",
    "        # Encoder for z outputs parameters for a normal distribution\n",
    "        # Final output size is [z_dim, z_dim] for mean and std dev, activations are None and Exp (for std)\n",
    "        self.encoder_z = MLP(\n",
    "            [self.input_size + 1] + hidden_sizes + [[z_dim, z_dim]],  # Added +1 for binary y\n",
    "            activation=nn.Softplus,\n",
    "            output_activation=[None, Exp],  # Exp to ensure positive standard deviation\n",
    "            allow_broadcast=self.allow_broadcast,\n",
    "            use_cuda=self.use_cuda,\n",
    "        )\n",
    "\n",
    "        # Decoder outputs probabilities for a multinomial distribution\n",
    "        # Assuming the total number of categories in x is defined by input_size\n",
    "        self.decoder = MLP(\n",
    "            [z_dim + 1] + hidden_sizes + [self.input_size],  # Adjust input for binary y\n",
    "            activation=nn.Softplus,\n",
    "            output_activation=nn.Softmax(dim=-1),  # Softmax to create a probability distribution\n",
    "            allow_broadcast=self.allow_broadcast,\n",
    "            use_cuda=self.use_cuda,\n",
    "        )\n",
    "\n",
    "        if self.use_cuda:\n",
    "            self.cuda()\n",
    "\n",
    "    def model(self, xs, es=None, ys=None):\n",
    "        \"\"\"\n",
    "        The model corresponds to the following generative process:\n",
    "        p(z) = normal(0,I)              # prior on latents\n",
    "        p(y|x) = Bernoulli(1/2.)     # which phase (semi-supervised)\n",
    "        p(x|y,z) = Multinomial(loc(y,z))   # output composition\n",
    "        loc is given by a neural network  `decoder`\n",
    "\n",
    "        :param xs: a batch of composition data\n",
    "        :param ys: (optional) a batch of the class labels i.e.\n",
    "                   phase corresponding to a given composition\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        # register this pytorch module and all of its sub-modules with pyro\n",
    "        pyro.module(\"ss_vae\", self)\n",
    "\n",
    "        batch_size = xs.size(0)\n",
    "        options = dict(dtype=xs.dtype, device=xs.device)\n",
    "        with pyro.plate(\"data\"):\n",
    "            # sample the latents from the constant prior distribution\n",
    "            # prior_loc = torch.zeros(batch_size, self.z_dim, **options)\n",
    "            prior_loc = torch.zeros(batch_size, self.z_dim, device=device)\n",
    "            # prior_scale = torch.ones(batch_size, self.z_dim, **options)\n",
    "            prior_scale = torch.ones(batch_size, self.z_dim, device=device)\n",
    "            zs = pyro.sample(\"z\", dist.Normal(prior_loc, prior_scale).to_event(1))\n",
    "\n",
    "            # if the label y (which phase) is unsupervised, sample from the\n",
    "            # constant prior, otherwise, observe the value (i.e. score it against the constant prior)\n",
    "            ys_prior_mean  = torch.ones(size=[batch_size, self.output_size], device=device) *0.5\n",
    "            if ys is None:\n",
    "                ys = pyro.sample(\"y\", dist.Bernoulli(probs=ys_prior_mean).to_event(1))\n",
    "            else:\n",
    "                ys = pyro.sample(\"y\", dist.Bernoulli(probs=ys_prior_mean).to_event(1), obs=ys)\n",
    "\n",
    "            # Finally, score the composition data (x) using the latent (z) and\n",
    "            # the class label y (which phase) against the\n",
    "            # parametrized distribution p(x|y,z) = Multinomial(decoder(y,z))\n",
    "            # where `decoder` is a neural network.\n",
    "            loc = self.decoder([zs, ys])\n",
    "            pyro.sample(\"x\", dist.Multinomial(total_count=1000, probs=loc), obs=xs)\n",
    "            # return the loc so we can visualize it later\n",
    "            return loc\n",
    "\n",
    "    def guide(self, xs, es=None, ys=None):\n",
    "        \"\"\"\n",
    "        The guide corresponds to the following:\n",
    "        q(y|x) = Bernoulli(probs(f(x)))              # infer phase from engineered features for composition x\n",
    "        q(z|x,y) = normal(loc(x,y),scale(x,y))       # infer latents from composition and the phase\n",
    "        loc, scale are given by a neural network `encoder_z`\n",
    "        probs is given by a neural network `encoder_y`\n",
    "\n",
    "        :param xs: a batch of composition data\n",
    "        :param es: a batch of engineered features for xs i.e. f(x) \n",
    "        :param ys: (optional) a batch of the class labels i.e.\n",
    "                   the phase corresponding to the composition(s)\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        # inform Pyro that the variables in the batch of xs, ys are conditionally independent\n",
    "        with pyro.plate(\"data\"):\n",
    "            # if the class label (the phase) is not supervised, sample\n",
    "            # (and score) the phase with the variational distribution\n",
    "            # q(y|x) =  Bernoulli(probs(f(x)))\n",
    "            if ys is None:\n",
    "                probs = self.encoder_y(es)\n",
    "                ys = pyro.sample(\"y\", dist.Bernoulli(probs=probs).to_event(1))\n",
    "\n",
    "            # sample (and score) the latent  with the variational\n",
    "            # distribution q(z|x,y) = normal(loc(x,y),scale(x,y))\n",
    "            loc, scale = self.encoder_z([xs, ys])\n",
    "            pyro.sample(\"z\", dist.Normal(loc, scale).to_event(1))\n",
    "\n",
    "    def classifier(self, es):\n",
    "        \"\"\"\n",
    "        classify engineered features of a composition (or a batch)\n",
    "\n",
    "        :param es: a batch of engineered features for a composition\n",
    "        :return: a batch of the corresponding class labels\n",
    "        \"\"\"\n",
    "        # use the trained model q(y|x) = Bernoulli(probs(f(x)))\n",
    "        \n",
    "        alpha = self.encoder_y(es)\n",
    "        ys = (alpha > 0.5).float()\n",
    "        \n",
    "        return ys\n",
    "\n",
    "    def model_classify(self, xs, es, ys=None):\n",
    "        \"\"\"\n",
    "        this model is used to add an auxiliary (supervised) loss as described in the\n",
    "        Kingma et al., \"Semi-Supervised Learning with Deep Generative Models\".\n",
    "        \"\"\"\n",
    "        # register all pytorch (sub)modules with pyro\n",
    "        pyro.module(\"ss_vae\", self)\n",
    "\n",
    "        # inform Pyro that the variables in the batch of xs, ys are conditionally independent\n",
    "        with pyro.plate(\"data\"):\n",
    "            # this here is the extra term to yield an auxiliary loss that we do gradient descent on\n",
    "            if ys is not None:\n",
    "                probs = self.encoder_y(es)\n",
    "                with pyro.poutine.scale(scale=self.aux_loss_multiplier):\n",
    "                    pyro.sample(\"y_aux\", dist.Bernoulli(probs=probs).to_event(1), obs=ys)\n",
    "\n",
    "    def guide_classify(self, xs, es, ys=None):\n",
    "        \"\"\"\n",
    "        dummy guide function to accompany model_classify in inference\n",
    "        \"\"\"\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "85785f91-1a83-43f2-801d-913e77bc082c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(data_loader, classifier_fn, batch_size, cuda=False):\n",
    "    \"\"\"\n",
    "    compute the accuracy over the supervised training set or the testing set\n",
    "    \"\"\"\n",
    "    predictions, actuals = [], []\n",
    "\n",
    "    # use the appropriate data loader\n",
    "    for xs, es, ys in data_loader:\n",
    "        if cuda:\n",
    "            ys = ys.cuda()\n",
    "            xs = xs.cuda()\n",
    "            es = es.cuda()\n",
    "        # use classification function to compute all predictions for each batch\n",
    "        predictions.append(classifier_fn(es))\n",
    "        actuals.append(ys)\n",
    "    actuals = torch.cat(actuals, dim=0).squeeze()\n",
    "    predictions = torch.cat(predictions, dim=0).squeeze()\n",
    "    \n",
    "    accuracy = (actuals == predictions).float().mean()\n",
    "    \n",
    "    return accuracy.item(), actuals, predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4621e79-bcda-4670-8ded-9fe183e53ab3",
   "metadata": {},
   "source": [
    "**load trained model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3237cd43-46ce-4bb4-8db0-de5d06d36b88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Args:\n",
    "    learning_rate = 1e-3#5e-4\n",
    "    num_epochs = 5000 #1000\n",
    "    hidden_layers = DEFAULT_HIDDEN_DIMS\n",
    "    z_dim = DEFAULT_Z_DIM\n",
    "    beta_1 = 0.900\n",
    "    aux_loss = True\n",
    "    aux_loss_multiplier = 50 #50.0\n",
    "    # cuda = True\n",
    "    cuda = False\n",
    "\n",
    "args = Args()\n",
    "\n",
    "\n",
    "pyro.clear_param_store()\n",
    "# if __name__ == '__main__':\n",
    "unsup_num = len(hea_feature_dataset_ul)\n",
    "sup_num = len(hea_feature_dataset)\n",
    "val_num = len(hea_feature_dataset_val)\n",
    "\n",
    "ssvae = SSVAE(output_size=1, input_size=FEATURE_SIZE,\n",
    "              z_dim=args.z_dim,\n",
    "              hidden_layers=args.hidden_layers,\n",
    "              use_cuda=args.cuda,\n",
    "              aux_loss_multiplier=args.aux_loss_multiplier)\n",
    "\n",
    "ssvae.load_state_dict(torch.load(\"./models/ssvae.model\", \n",
    "                                map_location=torch.device('cpu')))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "954f2425-660d-4639-87a1-e3c449a34971",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SSVAE(\n",
       "  (encoder_y): MLP(\n",
       "    (sequential_mlp): Sequential(\n",
       "      (0): ConcatModule()\n",
       "      (1): Linear(in_features=8, out_features=100, bias=True)\n",
       "      (2): Softplus(beta=1.0, threshold=20.0)\n",
       "      (3): Linear(in_features=100, out_features=100, bias=True)\n",
       "      (4): Softplus(beta=1.0, threshold=20.0)\n",
       "      (5): Linear(in_features=100, out_features=1, bias=True)\n",
       "      (6): Sigmoid()\n",
       "    )\n",
       "  )\n",
       "  (encoder_z): MLP(\n",
       "    (sequential_mlp): Sequential(\n",
       "      (0): ConcatModule()\n",
       "      (1): Linear(in_features=31, out_features=100, bias=True)\n",
       "      (2): Softplus(beta=1.0, threshold=20.0)\n",
       "      (3): Linear(in_features=100, out_features=100, bias=True)\n",
       "      (4): Softplus(beta=1.0, threshold=20.0)\n",
       "      (5): ListOutModule(\n",
       "        (0): Sequential(\n",
       "          (0): Linear(in_features=100, out_features=2, bias=True)\n",
       "        )\n",
       "        (1): Sequential(\n",
       "          (0): Linear(in_features=100, out_features=2, bias=True)\n",
       "          (1): Exp()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder): MLP(\n",
       "    (sequential_mlp): Sequential(\n",
       "      (0): ConcatModule()\n",
       "      (1): Linear(in_features=3, out_features=100, bias=True)\n",
       "      (2): Softplus(beta=1.0, threshold=20.0)\n",
       "      (3): Linear(in_features=100, out_features=100, bias=True)\n",
       "      (4): Softplus(beta=1.0, threshold=20.0)\n",
       "      (5): Linear(in_features=100, out_features=30, bias=True)\n",
       "      (6): Softmax(dim=-1)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ssvae"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86037c96-28dd-4748-b2c9-8678cbf19e4e",
   "metadata": {},
   "source": [
    "**prepare dataframes for further evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3f784297-c903-4da3-a94e-741e512b40e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = torch.tensor((test_hea.loc[:, \"Fe\":\"Sc\"].values)*100).float()\n",
    "test_labels = torch.tensor(test_hea['Class'].values).unsqueeze(-1).float()\n",
    "test_engg = torch.tensor(test_hea.loc[:, \"k\":\"delta_h_mix\"].values).float()\n",
    "if Args.cuda:\n",
    "    test_data = test_data.cuda()\n",
    "    test_labels = test_labels.cuda()\n",
    "    test_engg = test_engg.cuda()\n",
    "\n",
    "u_z_loc, u_z_scale = ssvae.encoder_z([test_data, test_labels])\n",
    "label_pred = ssvae.classifier(test_engg)\n",
    "\n",
    "test_hea['Predicted_Class'] = label_pred.data.numpy().squeeze().astype(int).astype(str)\n",
    "test_hea['z1'] = u_z_loc[:, 0].data.numpy() #this is the z_1 for test data \n",
    "test_hea['z2'] = u_z_loc[:, 1].data.numpy() #this is the z_2 for test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9700035c-f528-45b3-96fa-80bbfd42f688",
   "metadata": {},
   "outputs": [],
   "source": [
    "labelled_data = torch.tensor((labelled_hea.loc[:, \"Fe\":\"Sc\"].values)*100).float()\n",
    "labelled_labels = torch.tensor(labelled_hea['Class'].values).unsqueeze(-1).float()\n",
    "labelled_engg = torch.tensor(labelled_hea.loc[:, \"k\":\"delta_h_mix\"].values).float()\n",
    "\n",
    "t_z_loc, t_z_scale = ssvae.encoder_z([labelled_data, labelled_labels])\n",
    "t_label_pred = ssvae.classifier(labelled_engg)\n",
    "\n",
    "labelled_hea['Predicted_Class'] = t_label_pred.data.numpy().squeeze().astype(int).astype(str)\n",
    "labelled_hea['z1'] = t_z_loc[:, 0].data.numpy() #this is the z_1 for training data\n",
    "labelled_hea['z2'] = t_z_loc[:, 1].data.numpy() #this is the z_2 for training data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "joint-purpose",
   "metadata": {},
   "source": [
    "## Interpolation study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "knowing-liberty",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from numpy.linalg import norm\n",
    "\n",
    "def count_ele(alloy):\n",
    "    pattern = re.compile(r'([A-Z][a-z]*)(\\d*\\.*\\d*?(?=\\D|$))')\n",
    "    alloy_sep = pattern.findall(alloy)\n",
    "    alloy_sep = [(x, float(y)) if y else (x, 1) for x, y in alloy_sep]\n",
    "    return len(alloy_sep)\n",
    "test_hea['num_ele'] = test_hea['Alloys'].apply(count_ele)\n",
    "\n",
    "convert_indice = [3, 2, 0, 1, 4, 5, 6, 7]\n",
    "def alloy_reconstruct(alloy, ssvae=ssvae):\n",
    "    y = ssvae.encoder_y(torch.tensor(test_hea[test_hea['Alloys'] == alloy].loc[:, 'k':'delta_h_mix'].values).float())\n",
    "    z = torch.tensor(test_hea[test_hea['Alloys'] == alloy].loc[:, 'z1':'z2'].values).float()\n",
    "    _, __, old_alloy= calculate_compositions(alloy)\n",
    "    comp_ftr, _, __ = calculate_compositions(old_alloy)\n",
    "    comp_ftr = np.array(comp_ftr)\n",
    "    inv_comps = ssvae.decoder([z, y])\n",
    "    inv_alloy_comps = (torch.round(inv_comps, decimals=2)*100).data.numpy()[0]\n",
    "    new_alloy = ''.join([str(x)+str(int(y)) for x, y in zip(top30, inv_alloy_comps) if y > 0])\n",
    "    new_comp_ftr, _, __ = calculate_compositions(new_alloy)\n",
    "    new_comp_ftr = np.array(new_comp_ftr)\n",
    "    return old_alloy, new_alloy, comp_ftr, new_comp_ftr\n",
    "\n",
    "def generate_alloys(z, y, ssvae=ssvae):\n",
    "    z, y = torch.tensor(z).float(), torch.tensor(y).float().unsqueeze(-1)\n",
    "    inv_comps = ssvae.decoder([z, y])\n",
    "    inv_alloy_comps = (torch.round(inv_comps, decimals=2)*100).data.numpy()\n",
    "    new_alloy = ''.join([str(p1)+str(int(p2)) for p1, p2 in zip(top30, inv_alloy_comps) if p2 > 0])\n",
    "    new_comp_ftr, _, __ = calculate_compositions(new_alloy)\n",
    "    new_comp_ftr = np.array(new_comp_ftr)\n",
    "    return new_alloy, new_comp_ftr\n",
    "\n",
    "def calculate_y(alloy, ssvae):\n",
    "    ftr_vec = calculate_engineered_features(alloy)\n",
    "    y = ssvae.encoder_y(torch.tensor(ftr_vec).float())\n",
    "    return y.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incorporated-corpus",
   "metadata": {},
   "source": [
    "### Scanning the latent space by grid points "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "southern-avatar",
   "metadata": {},
   "outputs": [],
   "source": [
    "z1_s = np.linspace(-0.1, 0.1, num=5)\n",
    "z2_s = np.linspace(-0.5, -0.3, num=5)\n",
    "y = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "adverse-spanking",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ti67Mo9V1Nb2Zr8Ta12Hf1 Cr7Al41Ti12Mo7V4Nb21Zr4Ta3\n",
      "0.17856593430042267 0.0013507463736459613\n",
      "[-0.1, -0.5]\n",
      "Ti83Mo5Nb1Zr1Ta9W1 Cr10Al39Ti12Mo13V3Nb16Zr3Ta4\n",
      "1.0 0.0031771438661962748\n",
      "[-0.1, -0.45]\n",
      "Ti84Mo4Ta8W3 Cr14Al37Ti12Mo19V2Nb10Zr2Ta4Si1\n",
      "1.0 0.012690410949289799\n",
      "[-0.1, -0.4]\n",
      "Ti69Mo5Ta7W18 Cr18Al34Ti13Mo22V1Nb5Zr1Ta4Si2\n",
      "0.0005020626704208553 0.058258868753910065\n",
      "[-0.1, -0.35]\n",
      "Ti29Mo4Ta4W64 Cr23Al28Ti14Mo24V1Nb3Ta3Si4\n",
      "0.8921411633491516 0.15730080008506775\n",
      "[-0.1, -0.3]\n",
      "Ti54Mo15V2Nb6Ta18W2Hf4 Cr8Al20Ti18Mo12V7Nb21Zr9Ta5\n",
      "0.9598656892776489 0.003085965057834983\n",
      "[-0.05, -0.5]\n",
      "Ti61Mo11V1Nb3Ta18W5Hf1 Cr10Al19Ti19Mo20V4Nb14Zr7Ta6Si1\n",
      "0.5272950530052185 0.002405580598860979\n",
      "[-0.05, -0.45]\n",
      "Ti52Mo11V1Nb2Ta15W19 Cr12Al19Ti21Mo26V3Nb8Zr3Ta6Si2\n",
      "0.6226927042007446 0.00714917341247201\n",
      "[-0.05, -0.4]\n",
      "Ti22Mo7Nb1Ta7W63 Cr15Al18Ti23Mo27V2Nb4Zr1Ta5Si5\n",
      "0.8459881544113159 0.016039326786994934\n",
      "[-0.05, -0.35]\n",
      "Ti4Mo2Ta1W92 Cr17Al16Ti25Mo26V1Nb2Ta4W1Si9\n",
      "1.0 0.040373530238866806\n",
      "[-0.05, -0.3]\n",
      "Ti17Mo25V5Nb25Ta19W6Hf2 Cr6Al6Ti21Mo16V11Nb19Zr15Ta5Si1\n",
      "0.9036388993263245 0.010074862278997898\n",
      "[0.0, -0.5]\n",
      "Ti20Mo20V4Nb17Ta20W19 Cr8Al6Ti23Mo25V7Nb13Zr11Ta6Si2\n",
      "0.7555537223815918 0.016868557780981064\n",
      "[0.0, -0.45]\n",
      "Ti13Mo12V2Nb9Ta11W52 Cr9Al6Ti28Mo29V4Nb7Zr6Ta6Si4\n",
      "0.6346240639686584 0.03994619846343994\n",
      "[0.0, -0.4]\n",
      "Ti4Mo4Nb3Ta2W87 Cr11Al6Ti34Mo28V3Nb3Zr2Ta5Si9\n",
      "0.9997496008872986 0.03041885420680046\n",
      "[0.0, -0.35]\n",
      "Ti1Mo1Nb1W97 Cr13Al5Ti35Mo25V2Nb1Zr1Ta4W1Si14\n",
      "1.0 0.022220449522137642\n",
      "[0.0, -0.3]\n",
      "Ti2Mo19V6Nb55Ta8W9 Cr4Al2Ti19Mo17V15Nb16Zr20Ta4Hf1Si2\n",
      "0.060184404253959656 0.00780469598248601\n",
      "[0.05000000000000002, -0.5]\n",
      "Ti3Mo13V3Nb45Ta7W29 Cr5Al2Ti24Mo25V9Nb11Zr15Ta5Hf2Si3\n",
      "0.8449609279632568 0.00968000665307045\n",
      "[0.05000000000000002, -0.45]\n",
      "Ti2Mo6V1Nb26Ta3W62 Cr6Al1Ti32Mo27V7Nb6Zr8Ta5Hf2Si7\n",
      "0.6286217570304871 0.007060665171593428\n",
      "[0.05000000000000002, -0.4]\n",
      "Ti1Mo2Nb10Ta1W87 Cr8Al1Ti39Mo25V4Nb3Zr3Ta4Hf1Si12\n",
      "0.999894380569458 0.015430135652422905\n",
      "[0.05000000000000002, -0.35]\n",
      "Nb5W94 Cr11Al1Ti42Mo22V3Nb2Zr1Ta4W1Hf1Si13\n",
      "1.0 0.023767026141285896\n",
      "[0.05000000000000002, -0.3]\n",
      "Mo6V2Nb83Ta1W7 Cr3Ti16Mo16V17Nb13Zr20Ta2Hf10Si2\n",
      "1.0 0.00806952640414238\n",
      "[0.1, -0.5]\n",
      "Mo4V1Nb75Ta1W19 Cr3Ti20Mo22V12Nb9Zr12Ta3Hf14Si4\n",
      "0.3085131049156189 0.0036654863506555557\n",
      "[0.1, -0.45]\n",
      "Mo2Nb55W43 Cr5Ti29Mo24V9Nb6Zr4Ta4Hf11Si8\n",
      "0.6662639379501343 0.001432409044355154\n",
      "[0.1, -0.4]\n",
      "Nb32W67 Cr7Ti37Mo24V6Nb4Zr1Ta3Hf6Si11\n",
      "0.9995753169059753 0.011182764545083046\n",
      "[0.1, -0.35]\n",
      "Nb19W80 Cr15Ti38Mo20V6Nb4Ta4W1Hf4Si6\n",
      "0.9999943971633911 0.13409921526908875\n",
      "[0.1, -0.3]\n"
     ]
    }
   ],
   "source": [
    "from itertools import product\n",
    "for z1, z2 in product(z1_s, z2_s):\n",
    "    z = [z1, z2]\n",
    "    new_alloy, _ = generate_alloys(z, y)\n",
    "    uncertain_new_alloy, _ = generate_alloys(z, 0.5)\n",
    "    inv_new_alloy, _ = generate_alloys(z, 1-y)\n",
    "    y_new = calculate_y(new_alloy, ssvae)\n",
    "    y_inv = calculate_y(inv_new_alloy, ssvae)\n",
    "#     print(new_alloy, uncertain_new_alloy, inv_new_alloy)\n",
    "    print(new_alloy, inv_new_alloy)\n",
    "    print(y_new, y_inv)\n",
    "    print(z)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
